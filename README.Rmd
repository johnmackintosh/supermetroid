---
output: 
  github_document:
    toc: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  # cache = TRUE, # does this always cause an error?
  error = TRUE
)
```

# supermetroid

<!-- badges: start -->
<!-- badges: end -->

This is an analysis repository that captures Super Metroid speed run data from leaderboards and aims to analyse the data to answer the following question, posed by a runner. 

> At what times do Super Metroid 100% speed runs get competitive?

# What is Super Metroid?

> While the exploration-focused platformers known as Metroidvanias derive their genre name from two different series, there’s one game they’re all judged by. Super Metroid wasn’t the first Metroidvania, the first Metroid game also had the focus on exploring a large interconnected map and using new abilities to open up new areas, but its polish, visual direction, and atmosphere all created a more involved experience than its predecessors. Released in 1994, Super Metroid’s shadow looms over every game in the genre since... - [thegamehoard 2022](https://thegamehoard.com/2022/04/24/50-years-of-video-games-super-metroid-snes/) 


```{r fig.align='center', out.width="50%"}
# source: wikipedia
knitr::include_graphics("img/Smetroidbox-wiki.png")

```

## Super Metroid speed running


As part of the verification of their speed run results, many players upload a video of their run to youtube.

<iframe width="560" height="315" src="https://www.youtube.com/embed/7-cj22T2Yu4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>




# Analyses (work in progress)

```{r message=FALSE}
library(supermetroid)
library(tidyverse) # data science tools
library(gt) # for html tables


```


# Super Metroid and speed running


## Super Metroid is the top SNES speed runner game

```{r}
top_five_vis(base_size = 10)

```

- [ ] convert to coloured barchart, grouped by game, coloured by category
- [ ] can we scrape these data?

## Subway Surfers TikTok phenomenon or good upload interface?

```{r}
top_games_vis(base_size = 10)
```

- [ ] add platform


# Run times

### Speed run times from speedrun.com

```{r }

all_run_raincloud(src_run_df)

```

- [ ] interpretable x axis

# Segment times

## Label chaos

## Comparison of segments

## Single segment analyses



# Getting the data

## Sources

There are several sites where players upload Super Metroid speed running data.

site | api | description (why we're interested, these assumptions should be checked) | runs | players
- | - | - 
[speedrun](https://www.speedrun.com/supermetroid?h=Any&x=9d8v96lk)| [python](https://github.com/blha303/srcomapi)(how to get historical?) & [curl](https://github.com/glacials/splits-io/blob/master/docs/api.md) | most complete list of speed runs
[splitio](https://splits.io/) | [python](https://github.com/splitio/python-api) | by-segment times for each run, but for a smaller number of runners than speedrun.com
[deertier](https://deertier.com/) | there is an api, but I think we can get everything we need with `rvest` | super metroid game-specific speed running site 

```{r}

tribble(
  ~source, ~runs, ~players,
  "speedrun.com", nrow(src_run_df), NA,
  "splits.io", NA, NA,
  "deertier", NA, NA
) %>% 
  gt()

```



## Desired output: `ggplot`-friendly/tidy data

Dataframes that describe runs, runners, and, eventually, categories. Ideally, aggregated across the datasets. 

However, this will results in missing data, so specific analyses need to take that into account, or use source-specific datasets from the aggregated set with caveats. Linking ids are in italics.

Either way, we need to know what we have in each dataset, and have a universal schema across datasets.

### Runs 

Each row describes one speed run; a player may have multiple runs in a database. 

run_df | description 
- | - 
date | timestamp of run upload
*run_id* | unique identifier of run
run_time | total time of run in s or ms
*player_id* | unique identifier of player
rank | rank of player

To build a speed run leaderboard across datasets, we'd need to do some fancy engineering, so we will focus on source-specific analyses for now. Each leaderboard comprises runs.

run_df | description 
- | - 
date | timestamp of run upload
*run_id* | unique identifier of run
run_time | total time of run in s or ms
*player_id* | unique identifier of player
rank | rank of player
*src_id* | speedrun.com id
*sio_id* | splits.io id
*deertier_id* | deertier id
historical | if a historical ranking, need to inspect data to see how this should be recorded, possibly a bool, or putting NA in rank   

### Location of players on speedrun.com

player_df | description
- | -
*player_id* | unique identifier of player
player_handle | human-readable **unique** tag
location | geographic location of player

### Segments for each run from splitsio

segment_df | description
- | - 
*run_id* | unique identifier of run
segment_id | unique identifier of segment
t_s | time in seconds, measured to millisecond precision

# Import pkg & helper functions

```{python py pkg}
import srcomapi, srcomapi.datatypes as dt
import pandas as pd
import pickle
import splitsio

```

```{r r pkg, message=FALSE}
library(tidyquant)
library(ggdist) # for rainclouds
library(reticulate) # Python <-> R

# load r functions for this analysis
library(supermetroid)


```


```{python}
# helper function: saving an anonymous object in python 
# https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence

def save_object(obj, filename):
    with open(filename, 'wb') as outp:  # Overwrites any existing file.
        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)

```

## Last updated

```{r eval=FALSE}
# this chunk is evaluated when data is updated

write_rds(today(), "data-raw/readme-updated.rds")

```

```{r}
# get last updated

update_date <- read_rds("data-raw/readme-updated.rds")

```

This read me was last updated: `r update_date`.

# Get data from speedrun.com

## Load runs from 100% leaderboard

The game object was too nested to pickle, so runs have been extracted and saved.

```{python}
# get the runs object wrangled above

# data extracted in data-raw/get-src.py

with open('data-raw/src_runs.pkl', 'rb') as inp:
    src_runs = pickle.load(inp)
    
```


```{python}
# inspect runs object
type(src_runs)
len(src_runs)

```


```{python}

# inspect one element
src_runs[0] 
type(src_runs[0])
src_runs[0].keys()

# run element is a nested dictionary
type(src_runs[0]['run'])
src_runs[0]['run'].keys()


```


```{python}

# inspect a few elements
src_runs[0:3] 

```


## Run dataframe

src_run_df | description | from
- | - | - 
date | timestamp of run upload | `runs[index]['run']['date']`
*run_id* | unique identifier of run | `runs[index]['run']['date']`
run_time | total time of run in s or ms | `runs[index]['run']['times']['realtime_t']`
*player_id* | unique identifier of player | player id == run id, debug extraction
rank | rank, empty if historical, currently don't have historical obs | `src_runs[x]['place']`
 
```{python}

n_obs = len(src_runs)

# how to get splits? [src_runs][x]['run']['splits']

# extract elements with list comprehension
src_run_df = pd.DataFrame({
  'run_id' : [src_runs[x]['run']['id'] for x in range(n_obs)],
  # need to inspect what happens to historical ranking
  'rank' : [src_runs[x]['place'] for x in range(n_obs)], 
  't_s' : [src_runs[x]['run']['times']['realtime_t'] for x in range(n_obs)],
  'date' : [src_runs[x]['run']['date'] for x in range(n_obs)],
  'player' : [pd.DataFrame(src_runs[x]['run']['players']).iloc[0,1] for x in range(n_obs)]
})

# inspect runs dataframe
src_run_df.head()

```


```{python}
# number of unique values
pd.DataFrame(src_runs[1]['run']['players']).loc['id']


```

## Write speedrun.com data to package

```{r}
# get the data from python into R

src_run_raw <- py$src_run_df

```

## Visualise runs

Now we have the data from speedrun.com leaderboard, we can plot the distribution of runs. 

```{r}
all_run_raincloud(src_run_raw)

```

We have a handful of 0 entries and some > 3 hours. 

```{r}
# how many runs are really low?
src_run_raw %>% 
  filter(t_s < 4000 | t_s > 3 * 60 * 60)

```

The 0 entries are run times where `gametime` was captured, but `realtime` was not by the tool the player used to record the run. Our analyses are on `realtime`, so we will exclude these observations. 

We are also interest in comparing _speed runners_, as opposed to those logging playing through the game, which takes some hours. 

```{r}
(prop_greater_3hrs <- sum(src_run_raw$t_s > 3 * 60 * 60) / nrow(src_run_raw))

```

Since only {r round(prop_greater_3hrs, 2) * 100}% of runs are greater than 3 hours, these are negligible, and arguably not _speed_ runs. We will define a Super Metroid speed run, for this analysis, to be a Super Metroid 100% run that takes under 3 hours. 

## Raincloud

```{r}
src_run_raw %>%
  filter(
    # remove 0 length runs
    t_s > 0, 
    # exclude runs > 3 hours
    t_s < 3 * 60 * 60) %>%  
  all_run_raincloud()

```

## Write run data from speedrun.com to supermetroid

```{r eval=FALSE}
# this chunk is evaluated when data is updated
src_run_df <- 
  src_run_raw %>% 
  filter(t_s > 0,t_s < 3 * 60 * 60)


usethis::use_data(src_run_df, overwrite=TRUE)

```



# Getting the data from splitsio

## Load runs and runners  

```{python eval=FALSE}
# this chunk is not evaluated 
# to minimise api calls

# get 100% Category Super Metroid game data  
hun_cat = splitsio.Category.from_id("279", historic=True)
save_object(hun_cat, "data-raw/sio_hun_cat.pkl")

```


```{python load splitsio dat}
# load sio data
with open('data-raw/sio_hun_cat.pkl', 'rb') as inp:
    hun_cat = pickle.load(inp)

# extract runners and run from category
sio_runners = hun_cat.runners()

sio_runs = hun_cat.runs()

```

## Inspect data

```{python}
# take a look at a run
sio_runs[0]

# can we call nested elements by index and key?
type(sio_runs[0])
# I think this is a series object?
print(pd.DataFrame(sio_runs[1:3]))

print(pd.DataFrame(sio_runs[1:3]).columns)

```


```{python }
# take a look at a runner
sio_runners[0]

```

## Run dataframe

Objective: to wrangle a data frame with run data

sio_run_df | description | splitsio 
- | - | - 
*run_id* | splits.io id | `id` | 
date | timestamp of run upload | ? `'created_at', 'parsed_at', 'updated_at'` 
run_time | total time of run in s or ms | `realtime_duration_ms`
rank | "historical" if from previous record (nb only applies to speedrun.com); otherwise rank as int, list column | ?
src_player_id | speedrun.com player id | `srdc_id`

## Convert splitsio object to dataframe

```{python, eval=FALSE}
# this chunk isn't evaluated

# this dataframe takes too long to create for knitting, save object
sio_df = pd.DataFrame(sio_runs)

save_object(sio_df, "data-raw/sio_df.pkl")



```

## Load splitsio data

```{python load runners}

# load runs df
with open('data-raw/sio_df.pkl', 'rb') as inp:
    sio_df = pickle.load(inp)


```

```{python}
# take a look 
sio_df.columns

sio_df.head()
```

## 3 different fields for dates

```{python}

# three fields that look like dates
# which should we intepret as the "date" of the run?
sio_df[['created_at', 'parsed_at', 'updated_at']]

```

### Run id

```{python}
# id is hopefully the run id

# there are the same number of unique run 
# ids as there are rows in the dataframe
sio_df[['id']].drop_duplicates().shape[0] == sio_df.shape[0]

# so we can assume id is run id

```

### speedrun.com player id

```{python}
# it would be super useful to have the srcom ids match

# this many are not empty

sio_df[['srdc_id']].dropna() 

# that means we can only match this proportion of records, possibly we can use
# play handle

# why are some srdc ids duplicated? because historic=TRUE for these data,
# so these must be speedrun.com player ids

```

Only 10% of records contain speedrun.com ids, though:

```{python}
src_splits_prop = sio_df[['id','srdc_id']].dropna().shape[0] / sio_df.shape[0]

round(src_splits_prop, 2)

```

## srdc ids and player ids?

### ids in speedrun.com

```{python}
src_run_df[['run_id', 'play_id']]

```

### ~10% ids in splits.io

```{python}
sio_df[['srdc_id']].dropna() 

```



## create segments dataframe

segment_df | description
- | - 
*run_id* | unique identifier of run
segment_id | unique identifier of segment
t_s | time in seconds, measured to millisecond precision

```{python}
# first level of segments
type(sio_df['segments'][0])
len(sio_df['segments'][0])

# second level of segments
type(sio_df['segments'][0][0])
sio_df['segments'][0][2].keys()


segments_list = [pd.DataFrame(sio_df.segments[x]).assign(run_id = sio_df.id[x]) for x in range(sio_df.shape[0]) ]

segment_raw = pd.concat(segments_list)

segment_raw.columns

```

## fucking segment labels

This could easily be its own vignette, it's so complicated.

```{r}



```


# deertier


# speedrunslive




# wikipedia


# later

- [ ] You'll still need to render `README.Rmd` regularly, to keep `README.md` up-to-date. `devtools::build_readme()` is handy for this. You could also use GitHub Actions to re-render `README.Rmd` every time you push. An example workflow can be found here: <https://github.com/r-lib/actions/tree/v1/examples>.


